---
title: Weekly Notes – 18/7/15
author: nicr9
type: post
date: 2015-07-19T10:18:48+00:00
url: /?p=10
featured_image: /wp-content/uploads/2015/07/docker-machine.png
tags:
  - docker
---

This is the first post in a series that I'll (hopefully) send out every week. It'll cover the problems I have during the week as well as how I overcome those issues.

Hopefully this will be of some use to people out there, leave me a comment if you find any of this useful!

## Docker machine + docker compose

![](/wp-content/uploads/2015/07/docker-machine.png)

So I've been playing around with [`docker-machine`][1] while working on a pet project of mine: [daftpunk][2].

For those of you that aren't familiar: `docker-machine` is a tool to manage hosts running the docker-engine (the server part of docker that actually manages containers). You can use it to create VMs locally using virtual box or vmware that run boot2docker. It can also create instances on various cloud providers and have them all set up and ready to go in minutes. It's been really handy, I've used it to create an EC2 instance to host all the components of my project as containers.

Because daftpunk sports a web front end (written using flask), I thought it would be cool to mount all the flask-related code as a shared volume. That way I could make changes locally and they would be synced to the container hosting the flask app in AWS. Since changes to a flask app would reload the debug server automatically it seemed like a pretty neat way of testing out any changes I was making; at least, in theory. Here's a mock up of the docker-compose config I'd need to get this to work:

```yaml
web:
 build: frontend
 volumes:
 - frontend: /opt/frontend
 ports:
 - 5000:5000
```

It worked just fine when I ran it against my local docker-engine but I spent some time wrestling with the setup when I switched to AWS. The flask server wouldn't start and when I investigated it seemed that the mount point in the container was empty. Eventually it dawned on me. You just can't mount files/folders to a remote container.

This makes perfect sense once I stopped to think about it for a minute. What would happen if I turned off my local box? Let's say for the sake of the argument that the remote container would keep the files it had; those files might be important to it's operation after all. But things would get complicated if I tried to reconnect to the remote engine. Should it try to find the same stuff on my host and sync with it? This raises questions about merging file changes and that's really the realm of [DVCS][3]' like git. Too complicated. Much simpler to just leave that functionality out when working with remote docker-engines.

So what's the best way to reload the web front end? Well first I needed to add the front end code when I'm building the image instead of mounting it when I'm running the container. In the `docker-compose.yaml` file I just remove the volumes section and I add this to frontend/`Dockerfile`:

```dockerfile
ADD . /opt/frontend
```

Then any time I want to test new changes I can use docker-compose to rebuild and redeploy only the front end container, like so:

```bash
$ docker-compose build web
$ docker-compose up web
```

This takes a little bit longer but I don't have to worry about syncing files to remote hosts.

 [1]: https://github.com/docker/machine
 [2]: https://github.com/nicr9/daftpunk
 [3]: https://en.wikipedia.org/wiki/Distributed_revision_control
